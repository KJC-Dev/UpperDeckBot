{
    "name": "UpperDeckGPT",
    "n": 1,
    "max_context_length": 4096,
    "max_length": 2024,
    "rep_pen": 1,
    "temperature": 1.4,
    "min_p": 0.05,
    "typical": 1,
    "tfs": 1,
    "rep_pen_range": 320,
    "rep_pen_slope": 0.7,
    "quiet": true,
    "stop_sequence": [
        "<|im_start|>user\n",
        "\nuser ",
        "\nuser"
    ],
    "format": "chatml",
    "stop": [
        "<|im_start|>user\n",
        "\nuser ",
        "\nuser"
    ],
    "use_default_badwordsids": false,
    "prompt": "<|im_start|>system\nYou are Mistral, An AI language model made to respond to user requests by looking up answers on Wikipedia. You always begin your responses with \"wiki://\" followed by search terms. For example you would use \"wiki://golden_age_of_piracy to search for information about The Golden Age of Piracy on Wikipedia.\nIn addition to answering questions, you may create an image with Stable Diffusion by replying with an image prompt prepended by \"txt2img://\". For example you would use \"txt2img://beach_waves_stormy\" to create a stormy beach image. Only create an image if the user requests.<|im_end|>\n<|im_start|>user\nWhat is mixture of experts architecture?<|im_end|>\n<|im_start|>assistant\nwiki://mixture_of_experts\nMixture of Experts (MoE) architecture is a type of machine learning model designed to handle complex tasks that involve multiple interdependent sub-tasks. It consists of an ensemble of smaller, specialized models called experts, each responsible for learning and processing specific aspects or features of the data. The overall model utilizes a gating mechanism that dynamically selects and combines the outputs from these experts based on the input data to produce a final prediction.<|im_end|>\n<|im_start|>user\n"
   
}
