{
    "name": "UpperDeckGPT",
    "n": 1,
    "max_context_length": 4096,
    "max_length": 2024,
    "rep_pen": 1,
    "temperature": 1.4,
    "min_p": 0.05,
    "typical": 1,
    "tfs": 1,
    "rep_pen_range": 320,
    "rep_pen_slope": 0.7,
    "quiet": true,
    "stop_sequence": [
        "<|im_start|>user\n",
        "\nuser ",
        "\nuser"
    ],
    "format": "chatml",
    "stop": [
        "<|im_start|>user\n",
        "\nuser ",
        "\nuser"
    ],
    "use_default_badwordsids": false,
    "prompt": "<|im_start|>system\nYou are Gemma, An AI language model made to respond to user requests.In addition to answering questions, you may create an image with Stable Diffusion by replying with an image prompt prepended by \"txt2img://\". For example you would use \"txt2img://beach_waves_stormy\" to create a stormy beach image. Only create an image if the user requests.<|im_end|>\n<|im_start|>user\nWhat is mixture of experts architecture?<|im_end|>\n<|im_start|>assistant\nwiki://mixture_of_experts\nMixture of Experts (MoE) architecture is a type of machine learning model designed to handle complex tasks that involve multiple interdependent sub-tasks. It consists of an ensemble of smaller, specialized models called experts, each responsible for learning and processing specific aspects or features of the data. The overall model utilizes a gating mechanism that dynamically selects and combines the outputs from these experts based on the input data to produce a final prediction.<|im_end|>\n<|im_start|>user\n"
   
}
